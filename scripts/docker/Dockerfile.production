FROM crawl4ai-base-amd64

# Set working directory
WORKDIR /app

# Install PostgreSQL client, wget, and certificates
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq-dev \
    postgresql-client \
    wget \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Cloud SQL Proxy
RUN wget https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.9.0/cloud-sql-proxy.linux.amd64 -O /usr/local/bin/cloud-sql-proxy \
    && chmod +x /usr/local/bin/cloud-sql-proxy

# Copy requirements.txt first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Install Playwright browser
RUN playwright install chromium && playwright install-deps

# Copy application code and data files
COPY crawler_job /app/crawler_job
COPY database_migrations /app/database_migrations
COPY alembic.ini /app/alembic.ini
COPY recipients.txt /app/recipients.txt

# Set environment variables
ENV PYTHONPATH=/app
# Environment variable for verbose mode (can be overridden at runtime)
ENV CRAWLER_VERBOSE=False
# Default to Vesteda (can be overridden at runtime)
ENV CRAWLER_WEBSITE=Vesteda

# Copy scripts for database migration and configuration
COPY scripts/shell/insert_vesteda_config.py /app/scripts/insert_vesteda_config.py

# Create directories for logs and browser data
RUN mkdir -p /app/logs
RUN mkdir -p /app/browser_data/vesteda

# Create start script for production with Cloud SQL Proxy and Chrome
RUN echo '#!/bin/bash\n\
    # Start Cloud SQL Proxy if CLOUD_SQL_INSTANCE is set\n\
    PROXY_PID=""\n\
    if [ -n "$CLOUD_SQL_INSTANCE" ]; then\n\
    echo "Starting Cloud SQL Proxy for instance: $CLOUD_SQL_INSTANCE..."\n\
    # Start Cloud SQL Proxy in background\n\
    /usr/local/bin/cloud-sql-proxy --port=5432 "$CLOUD_SQL_INSTANCE" &\n\
    PROXY_PID=$!\n\
    \n\
    # Wait for proxy to start up\n\
    echo "Waiting for Cloud SQL Proxy to start..."\n\
    sleep 5 # Adjust sleep time if necessary\n\
    \n\
    # Check if proxy started successfully\n\
    if ! kill -0 $PROXY_PID > /dev/null 2>&1; then\n\
    echo "Cloud SQL Proxy failed to start."\n\
    exit 1\n\
    fi\n\
    echo "Cloud SQL Proxy started successfully."\n\
    # Override database host to use local proxy\n\
    export POSTGRES_HOST=127.0.0.1\n\
    else\n\
    echo "CLOUD_SQL_INSTANCE not set. Skipping Cloud SQL Proxy."\n\
    # Ensure POSTGRES_HOST is set if not using proxy\n\
    if [ -z "$POSTGRES_HOST" ]; then\n\
    echo "Error: POSTGRES_HOST must be set if not using CLOUD_SQL_INSTANCE."\n\
    exit 1\n\
    fi\n\
    fi\n\
    \n\
    # Launch chrome in background with remote debugging\n\
    echo "Starting Chrome with remote debugging..."\n\
    \n\
    # Get path to the Chromium executable\n\
    CHROMIUM_PATH=$(find /root/.cache/ms-playwright -name chrome -type f | head -n 1)\n\
    \n\
    # Launch Chromium with remote debugging\n\
    if [ -n "$CHROMIUM_PATH" ]; then\n\
    $CHROMIUM_PATH --headless --disable-gpu --remote-debugging-address=0.0.0.0 --remote-debugging-port=9222 --no-sandbox &\n\
    CHROME_PID=$!\n\
    # Wait for Chrome to start\n\
    echo "Waiting for Chrome to start..."\n\
    sleep 5 # Adjust sleep time if necessary\n\
    if ! kill -0 $CHROME_PID > /dev/null 2>&1; then\n\
    echo "Chrome failed to start."\n\
    # Decide if Chrome failing is critical - exiting for now\n\
    # If proxy was started, clean it up\n\
    if [ -n "$PROXY_PID" ]; then kill $PROXY_PID; fi\n\
    exit 1\n\
    fi\n\
    echo "Chrome started successfully."\n\
    else\n\
    echo "Chromium executable not found. Cannot start Chrome."\n\
    # Decide if Chrome failing is critical - exiting for now\n\
    # If proxy was started, clean it up\n\
    if [ -n "$PROXY_PID" ]; then kill $PROXY_PID; fi\n\
    exit 1\n\
    fi\n\
    \n\
    # Run database migrations\n\
    echo "Running database migrations..."\n\
    cd /app && alembic upgrade head\n\
    MIGRATION_EXIT_CODE=$?\n\
    if [ $MIGRATION_EXIT_CODE -ne 0 ]; then\n\
    echo "Database migration failed with exit code $MIGRATION_EXIT_CODE"\n\
    # Cleanup and exit\n\
    if [ -n "$CHROME_PID" ]; then kill $CHROME_PID; fi\n\
    if [ -n "$PROXY_PID" ]; then kill $PROXY_PID; fi\n\
    exit $MIGRATION_EXIT_CODE\n\
    fi\n\
    echo "Database migrations completed successfully."\n\
    \n\
    # Insert Vesteda configuration if needed\n\
    echo "Ensuring Vesteda configuration exists..."\n\
    python /app/scripts/insert_vesteda_config.py\n\
    CONFIG_EXIT_CODE=$?\n\
    if [ $CONFIG_EXIT_CODE -ne 0 ]; then\n\
    echo "Configuration setup failed with exit code $CONFIG_EXIT_CODE"\n\
    # Cleanup and exit\n\
    if [ -n "$CHROME_PID" ]; then kill $CHROME_PID; fi\n\
    if [ -n "$PROXY_PID" ]; then kill $PROXY_PID; fi\n\
    exit $CONFIG_EXIT_CODE\n\
    fi\n\
    echo "Configuration setup completed successfully."\n\
    \n\
    # Run the crawler using the new main.py script\n\
    echo "Starting crawler for website: $CRAWLER_WEBSITE..."\n\
    python -m crawler_job.main --website="$CRAWLER_WEBSITE"\n\
    CRAWLER_EXIT_CODE=$?\n\
    echo "Crawler finished with exit code $CRAWLER_EXIT_CODE"\n\
    \n\
    # Cleanup: Stop Chrome\n\
    echo "Stopping Chrome..."\n\
    if [ -n "$CHROME_PID" ]; then kill $CHROME_PID; fi\n\
    \n\
    # Cleanup: Stop Cloud SQL Proxy if it was started\n\
    if [ -n "$PROXY_PID" ]; then\n\
    echo "Stopping Cloud SQL Proxy..."\n\
    kill $PROXY_PID\n\
    wait $PROXY_PID 2>/dev/null\n\
    fi\n\
    \n\
    exit $CRAWLER_EXIT_CODE\n\
    ' > /app/start.sh && chmod +x /app/start.sh

# Default command to run the start script
CMD ["/app/start.sh"] 